#!/usr/bin/env python3
"""PTT è‚¡æ¿æƒ…ç·’åˆ†æ â€” å— ICE Reddit Signals and Sentiment å•Ÿç™¼ã€‚

ç”¨æ³•:
    python main.py                              # åŸºæœ¬æƒ…ç·’åˆ†æ
    python main.py --pages 3 --json             # å¤šé  + JSON è¼¸å‡º
    python main.py --update-aliases             # å…ˆæ›´æ–°å‹•æ…‹æš±ç¨±å†åˆ†æ
    python main.py --contrarian                 # åæŒ‡æ¨™åµæ¸¬ (ç•¢æ¥­æ–‡/æ­å°)
    python main.py --buzz                       # ç•°å¸¸ç†±åº¦åµæ¸¬ (Pump-and-Dump é è­¦)
    python main.py --sectors                    # æ¿å¡Šè¼ªå‹•è¿½è¹¤
    python main.py --all                        # å…¨éƒ¨åˆ†æä¸€æ¬¡è·‘å®Œ
"""

import argparse
import json
import sys

from ptt_scraper import (
    BuzzDetector,
    EntityMapper,
    PttScraper,
    SectorTracker,
    SentimentScorer,
    summarize_contrarian,
    update_dynamic_aliases,
)


def main() -> None:
    parser = argparse.ArgumentParser(
        description="PTT Signals and Sentiment â€” çˆ¬å– PTT æ–‡ç« ä¸¦åˆ†ææƒ…ç·’",
    )
    parser.add_argument(
        "--board", default="Stock", help="ç›®æ¨™çœ‹æ¿ (é è¨­: Stock)",
    )
    parser.add_argument(
        "--pages", type=int, default=1, help="è¦çˆ¬å¹¾é  (é è¨­: 1)",
    )
    parser.add_argument(
        "--delay", type=float, default=0.5, help="æ¯æ¬¡è«‹æ±‚é–“éš”ç§’æ•¸ (é è¨­: 0.5)",
    )
    parser.add_argument(
        "--json", action="store_true", help="ä»¥ JSON æ ¼å¼è¼¸å‡ºçµæœ",
    )
    parser.add_argument(
        "--update-aliases", action="store_true",
        help="å¾ TWSE/TPEX æ›´æ–°å‹•æ…‹æš±ç¨±ï¼ˆè‚¡ç‹ã€è‚¡åç­‰ï¼‰",
    )
    parser.add_argument(
        "--contrarian", action="store_true",
        help="åæŒ‡æ¨™åµæ¸¬ï¼šç•¢æ¥­æ–‡æŒ‡æ•¸ / æ­å°æŒ‡æ•¸",
    )
    parser.add_argument(
        "--buzz", action="store_true",
        help="ç•°å¸¸ç†±åº¦åµæ¸¬ï¼šå€‹è‚¡è¨è«–é‡ Pump-and-Dump é è­¦",
    )
    parser.add_argument(
        "--sectors", action="store_true",
        help="æ¿å¡Šè¼ªå‹•è¿½è¹¤ï¼šä¸»é¡Œç†±åº¦æ’è¡Œ",
    )
    parser.add_argument(
        "--all", action="store_true",
        help="åŸ·è¡Œå…¨éƒ¨åˆ†æï¼ˆsentiment + contrarian + buzz + sectorsï¼‰",
    )
    args = parser.parse_args()

    if args.update_aliases:
        update_dynamic_aliases()
        print()

    # æ±ºå®šå•Ÿç”¨å“ªäº›åˆ†æ
    run_contrarian = args.contrarian or args.all
    run_buzz = args.buzz or args.all
    run_sectors = args.sectors or args.all
    run_sentiment = not (args.contrarian or args.buzz or args.sectors) or args.all

    # çˆ¬å–
    scraper = PttScraper(board=args.board, delay=args.delay)
    print(f"æ­£åœ¨çˆ¬å– PTT {args.board} ç‰ˆ (å…± {args.pages} é )...\n")
    posts = scraper.fetch_posts(max_pages=args.pages)

    if not posts:
        print("æœªæŠ“åˆ°ä»»ä½•æ–‡ç« ã€‚")
        sys.exit(0)

    output: dict = {}

    # 1. åŸºæœ¬æƒ…ç·’åˆ†æ
    if run_sentiment:
        scorer = SentimentScorer()
        mapper = EntityMapper()
        results = []
        for post in posts:
            sentiment = scorer.analyze_post(post)
            entities = mapper.find_entities(post.title + " " + post.content)
            results.append({
                "title": post.title,
                "url": post.url,
                "author": post.author,
                "date": post.date,
                "sentiment": {
                    "score": sentiment.score,
                    "label": sentiment.label,
                    "push": sentiment.push_count,
                    "boo": sentiment.boo_count,
                    "arrow": sentiment.arrow_count,
                },
                "entities": entities,
            })
        output["sentiment"] = results

    # 2. åæŒ‡æ¨™åµæ¸¬
    if run_contrarian:
        summary = summarize_contrarian(posts)
        output["contrarian"] = {
            "total_posts": summary.total_posts,
            "capitulation_count": summary.capitulation_count,
            "euphoria_count": summary.euphoria_count,
            "capitulation_ratio": round(summary.capitulation_ratio, 4),
            "euphoria_ratio": round(summary.euphoria_ratio, 4),
            "market_signal": summary.market_signal,
            "capitulation_posts": [
                {"title": s.title, "url": s.url, "hits": s.capitulation_hits}
                for s in summary.capitulation_posts
            ],
            "euphoria_posts": [
                {"title": s.title, "url": s.url, "hits": s.euphoria_hits}
                for s in summary.euphoria_posts
            ],
        }

    # 3. ç•°å¸¸ç†±åº¦åµæ¸¬
    if run_buzz:
        detector = BuzzDetector()
        report = detector.analyze(posts)
        detector.save_snapshot(posts)
        output["buzz"] = {
            "total_posts": report.total_posts,
            "tickers": [
                {
                    "ticker": t.ticker,
                    "name": t.name,
                    "mentions": t.mention_count,
                    "buzz_score": t.buzz_score,
                    "anomaly": t.is_anomaly,
                }
                for t in report.tickers
            ],
            "anomalies": [
                {"ticker": t.ticker, "name": t.name, "buzz_score": t.buzz_score}
                for t in report.anomalies
            ],
        }

    # 4. æ¿å¡Šè¼ªå‹•
    if run_sectors:
        tracker = SectorTracker()
        sector_report = tracker.analyze(posts)
        output["sectors"] = {
            "total_posts": sector_report.total_posts,
            "ranking": [
                {
                    "sector": h.sector,
                    "mentions": h.mention_count,
                    "keywords": h.matched_keywords,
                    "sample_titles": h.sample_titles,
                }
                for h in sector_report.sectors
            ],
        }

    # è¼¸å‡º
    if args.json:
        print(json.dumps(output, ensure_ascii=False, indent=2))
    else:
        _print_output(output)


# ------------------------------------------------------------------
# è¡¨æ ¼è¼¸å‡º
# ------------------------------------------------------------------

def _print_output(output: dict) -> None:
    if "sentiment" in output:
        _print_sentiment_table(output["sentiment"])

    if "contrarian" in output:
        _print_contrarian(output["contrarian"])

    if "buzz" in output:
        _print_buzz(output["buzz"])

    if "sectors" in output:
        _print_sectors(output["sectors"])


def _print_sentiment_table(results: list[dict]) -> None:
    print(f"\n{'='*90}")
    print("  æƒ…ç·’åˆ†æ (Sentiment)")
    print(f"{'='*90}")
    print(f"{'æ¨™é¡Œ':<40} {'æƒ…ç·’':>8} {'æ¨':>4} {'å™“':>4} {'â†’':>4} {'ç›¸é—œæ¨™çš„'}")
    print("-" * 90)
    for r in results:
        s = r["sentiment"]
        title = r["title"][:38]
        entities_str = ", ".join(
            f"{e['ticker']}({e['name']})" if e["name"] else e["ticker"]
            for e in r["entities"]
        )
        label_map = {
            "bullish": "ğŸŸ¢çœ‹å¤š",
            "bearish": "ğŸ”´çœ‹ç©º",
            "neutral": "âšªä¸­æ€§",
        }
        label = label_map.get(s["label"], s["label"])
        print(
            f"{title:<40} {label:>8} {s['push']:>4} {s['boo']:>4} {s['arrow']:>4} {entities_str}"
        )

    total = len(results)
    bullish = sum(1 for r in results if r["sentiment"]["label"] == "bullish")
    bearish = sum(1 for r in results if r["sentiment"]["label"] == "bearish")
    neutral = total - bullish - bearish
    print("-" * 90)
    print(f"å…± {total} ç¯‡ | çœ‹å¤š: {bullish} | çœ‹ç©º: {bearish} | ä¸­æ€§: {neutral}")


def _print_contrarian(data: dict) -> None:
    signal_map = {
        "extreme_fear": "ğŸ”´ æ¥µåº¦ææ…Œ (æ½›åœ¨åšå¤šè¨Šè™Ÿ)",
        "extreme_greed": "ğŸ”´ æ¥µåº¦è²ªå©ª (æ½›åœ¨éç†±è¨Šè™Ÿ)",
        "fear": "ğŸŸ¡ åææ…Œ",
        "greed": "ğŸŸ¡ åè²ªå©ª",
        "neutral": "âšª ä¸­æ€§",
    }

    print(f"\n{'='*90}")
    print("  åæŒ‡æ¨™åµæ¸¬ (Contrarian Indicator)")
    print(f"{'='*90}")
    print(f"å¸‚å ´è¨Šè™Ÿ: {signal_map.get(data['market_signal'], data['market_signal'])}")
    print(f"ç•¢æ¥­æ–‡: {data['capitulation_count']}/{data['total_posts']} "
          f"({data['capitulation_ratio']:.1%})")
    print(f"æ­å°æ–‡: {data['euphoria_count']}/{data['total_posts']} "
          f"({data['euphoria_ratio']:.1%})")

    if data["capitulation_posts"]:
        print("\nç•¢æ¥­æ–‡åˆ—è¡¨:")
        for p in data["capitulation_posts"]:
            print(f"  - {p['title']}")
            print(f"    é—œéµå­—: {', '.join(p['hits'])}")

    if data["euphoria_posts"]:
        print("\næ­å°æ–‡åˆ—è¡¨:")
        for p in data["euphoria_posts"]:
            print(f"  - {p['title']}")
            print(f"    é—œéµå­—: {', '.join(p['hits'])}")


def _print_buzz(data: dict) -> None:
    print(f"\n{'='*90}")
    print("  ç•°å¸¸ç†±åº¦åµæ¸¬ (Buzz Detector)")
    print(f"{'='*90}")

    if data["anomalies"]:
        print("âš ï¸  ç•°å¸¸æ¨™çš„:")
        for a in data["anomalies"]:
            name_str = f" ({a['name']})" if a["name"] else ""
            print(f"  ğŸ”¥ {a['ticker']}{name_str} â€” buzz score: {a['buzz_score']}")
        print()

    print(f"{'æ¨™çš„':<16} {'åç¨±':<12} {'æåŠ':>6} {'Buzz':>8} {'ç•°å¸¸':>6}")
    print("-" * 55)
    for t in data["tickers"][:15]:  # åªé¡¯ç¤ºå‰ 15 å
        name = t["name"][:10] if t["name"] else ""
        flag = "âš ï¸" if t["anomaly"] else ""
        print(f"{t['ticker']:<16} {name:<12} {t['mentions']:>6} {t['buzz_score']:>8.2f} {flag:>6}")


def _print_sectors(data: dict) -> None:
    print(f"\n{'='*90}")
    print("  æ¿å¡Šè¼ªå‹• (Sector Rotation)")
    print(f"{'='*90}")

    if not data["ranking"]:
        print("ï¼ˆæœªåµæ¸¬åˆ°ä»»ä½•æ¿å¡Šé—œéµå­—ï¼‰")
        return

    for i, s in enumerate(data["ranking"], 1):
        bar = "â–ˆ" * min(s["mentions"], 40)
        print(f"  {i:>2}. {s['sector']:<12} {bar} ({s['mentions']})")
        if s["keywords"]:
            print(f"      é—œéµå­—: {', '.join(s['keywords'][:5])}")
        if s["sample_titles"]:
            print(f"      ç¯„ä¾‹: {s['sample_titles'][0][:50]}")


if __name__ == "__main__":
    main()
