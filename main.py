#!/usr/bin/env python3
"""PTT / Reddit æƒ…ç·’åˆ†æ â€” å— ICE Reddit Signals and Sentiment å•Ÿç™¼ã€‚

ç”¨æ³•:
    python main.py                              # PTT åŸºæœ¬æƒ…ç·’åˆ†æï¼ˆé è¨­ï¼‰
    python main.py --all --pages 5              # PTT å…¨éƒ¨åˆ†æ
    python main.py --source reddit              # Reddit ç¾è‚¡/åŠ å¯†è²¨å¹£æƒ…ç·’
    python main.py --source reddit --subreddits wallstreetbets cryptocurrency
    python main.py --all --influxdb             # å…¨éƒ¨åˆ†æ + å¯«å…¥ InfluxDB
"""

import argparse
import json
import sys

from ptt_scraper import (
    BuzzDetector,
    EntityMapper,
    InfluxStore,
    PttScraper,
    SectorTracker,
    SentimentScorer,
    summarize_contrarian,
    update_dynamic_aliases,
)
from reddit_scraper import RedditEntityMapper, RedditScraper, RedditSentimentScorer


def main() -> None:
    parser = argparse.ArgumentParser(
        description="Signals and Sentiment â€” PTT / Reddit æƒ…ç·’åˆ†æ",
    )
    # è³‡æ–™æº
    parser.add_argument(
        "--source", choices=["ptt", "reddit"], default="ptt",
        help="è³‡æ–™æº (é è¨­: ptt)",
    )
    # PTT åƒæ•¸
    parser.add_argument(
        "--board", default="Stock", help="PTT çœ‹æ¿ (é è¨­: Stock)",
    )
    parser.add_argument(
        "--pages", type=int, default=1, help="PTT å¾€å‰çˆ¬å¹¾é  (é è¨­: 1)",
    )
    # Reddit åƒæ•¸
    parser.add_argument(
        "--subreddits", nargs="+", default=None,
        help="Reddit subreddit åˆ—è¡¨ (é è¨­: wallstreetbets stocks investing cryptocurrency bitcoin)",
    )
    parser.add_argument(
        "--limit", type=int, default=25,
        help="Reddit æ¯å€‹ subreddit æŠ“å¹¾ç¯‡ (é è¨­: 25, ä¸Šé™ 100)",
    )
    parser.add_argument(
        "--comments", action="store_true",
        help="Reddit: æ˜¯å¦é€²å…¥æ–‡ç« æŠ“ç•™è¨€ (è¼ƒæ…¢ä½†æ›´æº–ç¢º)",
    )
    # å…±ç”¨åƒæ•¸
    parser.add_argument(
        "--delay", type=float, default=None,
        help="æ¯æ¬¡è«‹æ±‚é–“éš”ç§’æ•¸ (PTT é è¨­ 0.5, Reddit é è¨­ 1.0)",
    )
    parser.add_argument(
        "--json", action="store_true", help="ä»¥ JSON æ ¼å¼è¼¸å‡ºçµæœ",
    )
    parser.add_argument(
        "--update-aliases", action="store_true",
        help="PTT: å¾ TWSE/TPEX æ›´æ–°å‹•æ…‹æš±ç¨±ï¼ˆè‚¡ç‹ã€è‚¡åç­‰ï¼‰",
    )
    parser.add_argument(
        "--contrarian", action="store_true",
        help="PTT: åæŒ‡æ¨™åµæ¸¬ï¼ˆç•¢æ¥­æ–‡ / æ­å°æ–‡ï¼‰",
    )
    parser.add_argument(
        "--buzz", action="store_true",
        help="ç•°å¸¸ç†±åº¦åµæ¸¬ï¼šå€‹è‚¡è¨è«–é‡ Pump-and-Dump é è­¦",
    )
    parser.add_argument(
        "--sectors", action="store_true",
        help="PTT: æ¿å¡Šè¼ªå‹•è¿½è¹¤",
    )
    parser.add_argument(
        "--all", action="store_true",
        help="åŸ·è¡Œå…¨éƒ¨åˆ†æ",
    )
    parser.add_argument(
        "--influxdb", action="store_true",
        help="å°‡çµæœå¯«å…¥ InfluxDBï¼ˆéœ€å…ˆ docker compose upï¼‰",
    )
    args = parser.parse_args()

    if args.source == "reddit":
        output = _run_reddit(args)
    else:
        output = _run_ptt(args)

    # å¯«å…¥ InfluxDB
    if args.influxdb:
        board_label = args.board if args.source == "ptt" else "reddit"
        store = InfluxStore()
        count = store.write_all(output, board_label)
        store.close()
        print(f"\nå·²å¯«å…¥ {count} ç­†è³‡æ–™åˆ° InfluxDBã€‚")

    # è¼¸å‡º
    if args.json:
        print(json.dumps(output, ensure_ascii=False, indent=2))
    else:
        _print_output(output)


# ------------------------------------------------------------------
# PTT åˆ†ææµç¨‹
# ------------------------------------------------------------------

def _run_ptt(args) -> dict:
    if args.update_aliases:
        update_dynamic_aliases()
        print()

    run_contrarian = args.contrarian or args.all
    run_buzz = args.buzz or args.all
    run_sectors = args.sectors or args.all
    run_sentiment = not (args.contrarian or args.buzz or args.sectors) or args.all

    delay = args.delay if args.delay is not None else 0.5
    scraper = PttScraper(board=args.board, delay=delay)
    print(f"æ­£åœ¨çˆ¬å– PTT {args.board} ç‰ˆ (å…± {args.pages} é )...\n")
    posts = scraper.fetch_posts(max_pages=args.pages)

    if not posts:
        print("æœªæŠ“åˆ°ä»»ä½•æ–‡ç« ã€‚")
        sys.exit(0)

    output: dict = {}

    if run_sentiment:
        scorer = SentimentScorer()
        mapper = EntityMapper()
        results = []
        for post in posts:
            sentiment = scorer.analyze_post(post)
            entities = mapper.find_entities(post.title + " " + post.content)
            results.append({
                "title": post.title,
                "url": post.url,
                "author": post.author,
                "date": post.date,
                "sentiment": {
                    "score": sentiment.score,
                    "label": sentiment.label,
                    "push": sentiment.push_count,
                    "boo": sentiment.boo_count,
                    "arrow": sentiment.arrow_count,
                },
                "entities": entities,
            })
        output["sentiment"] = results

    if run_contrarian:
        summary = summarize_contrarian(posts)
        output["contrarian"] = {
            "total_posts": summary.total_posts,
            "capitulation_count": summary.capitulation_count,
            "euphoria_count": summary.euphoria_count,
            "capitulation_ratio": round(summary.capitulation_ratio, 4),
            "euphoria_ratio": round(summary.euphoria_ratio, 4),
            "market_signal": summary.market_signal,
            "capitulation_posts": [
                {"title": s.title, "url": s.url, "hits": s.capitulation_hits}
                for s in summary.capitulation_posts
            ],
            "euphoria_posts": [
                {"title": s.title, "url": s.url, "hits": s.euphoria_hits}
                for s in summary.euphoria_posts
            ],
        }

    if run_buzz:
        detector = BuzzDetector()
        report = detector.analyze(posts)
        detector.save_snapshot(posts)
        output["buzz"] = {
            "total_posts": report.total_posts,
            "tickers": [
                {
                    "ticker": t.ticker,
                    "name": t.name,
                    "mentions": t.mention_count,
                    "buzz_score": t.buzz_score,
                    "anomaly": t.is_anomaly,
                }
                for t in report.tickers
            ],
            "anomalies": [
                {"ticker": t.ticker, "name": t.name, "buzz_score": t.buzz_score}
                for t in report.anomalies
            ],
        }

    if run_sectors:
        tracker = SectorTracker()
        sector_report = tracker.analyze(posts)
        output["sectors"] = {
            "total_posts": sector_report.total_posts,
            "ranking": [
                {
                    "sector": h.sector,
                    "mentions": h.mention_count,
                    "keywords": h.matched_keywords,
                    "sample_titles": h.sample_titles,
                }
                for h in sector_report.sectors
            ],
        }

    return output


# ------------------------------------------------------------------
# Reddit åˆ†ææµç¨‹
# ------------------------------------------------------------------

def _run_reddit(args) -> dict:
    delay = args.delay if args.delay is not None else 1.0
    scraper = RedditScraper(
        subreddits=args.subreddits,
        delay=delay,
        fetch_comments=args.comments,
    )
    subs_str = ", ".join(scraper.subreddits)
    print(f"æ­£åœ¨çˆ¬å– Reddit [{subs_str}] (æ¯ç‰ˆ {args.limit} ç¯‡)...\n")
    posts = scraper.fetch_posts(limit=args.limit)

    if not posts:
        print("æœªæŠ“åˆ°ä»»ä½•æ–‡ç« ã€‚")
        sys.exit(0)

    output: dict = {}

    # æƒ…ç·’åˆ†æ
    scorer = RedditSentimentScorer()
    mapper = RedditEntityMapper()
    results = []
    for post in posts:
        sentiment = scorer.analyze_post(post)
        entities = mapper.find_entities(post.title + " " + post.selftext)
        results.append({
            "title": post.title,
            "url": post.url,
            "author": post.author,
            "subreddit": post.subreddit,
            "sentiment": {
                "score": sentiment.score,
                "label": sentiment.label,
                "upvote_ratio": sentiment.upvote_ratio,
                "post_score": sentiment.post_score,
                "bullish_hits": sentiment.bullish_hits,
                "bearish_hits": sentiment.bearish_hits,
            },
            "entities": entities,
        })
    output["sentiment"] = results

    return output


# ------------------------------------------------------------------
# è¡¨æ ¼è¼¸å‡º
# ------------------------------------------------------------------

def _print_output(output: dict) -> None:
    if "sentiment" in output:
        _print_sentiment_table(output["sentiment"])

    if "contrarian" in output:
        _print_contrarian(output["contrarian"])

    if "buzz" in output:
        _print_buzz(output["buzz"])

    if "sectors" in output:
        _print_sectors(output["sectors"])


def _print_sentiment_table(results: list[dict]) -> None:
    is_reddit = bool(results and "subreddit" in results[0])
    label_map = {"bullish": "ğŸŸ¢Bull", "bearish": "ğŸ”´Bear", "neutral": "âšª----"}

    print(f"\n{'='*90}")
    print("  æƒ…ç·’åˆ†æ (Sentiment)")
    print(f"{'='*90}")

    if is_reddit:
        print(f"{'Title':<42} {'Signal':>8} {'Score':>6} {'Upvt%':>6} {'Tickers'}")
        print("-" * 90)
        for r in results:
            s = r["sentiment"]
            title = r["title"][:40]
            entities_str = ", ".join(
                f"{e['ticker']}({e['name']})" if e["name"] else e["ticker"]
                for e in r["entities"][:3]
            )
            label = label_map.get(s["label"], s["label"])
            ratio = f"{s['upvote_ratio']:.0%}"
            print(f"{title:<42} {label:>8} {s['score']:>6.1f} {ratio:>6} {entities_str}")
    else:
        print(f"{'æ¨™é¡Œ':<40} {'æƒ…ç·’':>8} {'æ¨':>4} {'å™“':>4} {'â†’':>4} {'ç›¸é—œæ¨™çš„'}")
        print("-" * 90)
        for r in results:
            s = r["sentiment"]
            title = r["title"][:38]
            entities_str = ", ".join(
                f"{e['ticker']}({e['name']})" if e["name"] else e["ticker"]
                for e in r["entities"]
            )
            label = label_map.get(s["label"], s["label"])
            print(
                f"{title:<40} {label:>8} {s['push']:>4} {s['boo']:>4} {s['arrow']:>4} {entities_str}"
            )

    total = len(results)
    bullish = sum(1 for r in results if r["sentiment"]["label"] == "bullish")
    bearish = sum(1 for r in results if r["sentiment"]["label"] == "bearish")
    neutral = total - bullish - bearish
    print("-" * 90)
    print(f"Total: {total} | Bullish: {bullish} | Bearish: {bearish} | Neutral: {neutral}")


def _print_contrarian(data: dict) -> None:
    signal_map = {
        "extreme_fear": "ğŸ”´ æ¥µåº¦ææ…Œ (æ½›åœ¨åšå¤šè¨Šè™Ÿ)",
        "extreme_greed": "ğŸ”´ æ¥µåº¦è²ªå©ª (æ½›åœ¨éç†±è¨Šè™Ÿ)",
        "fear": "ğŸŸ¡ åææ…Œ",
        "greed": "ğŸŸ¡ åè²ªå©ª",
        "neutral": "âšª ä¸­æ€§",
    }

    print(f"\n{'='*90}")
    print("  åæŒ‡æ¨™åµæ¸¬ (Contrarian Indicator)")
    print(f"{'='*90}")
    print(f"å¸‚å ´è¨Šè™Ÿ: {signal_map.get(data['market_signal'], data['market_signal'])}")
    print(f"ç•¢æ¥­æ–‡: {data['capitulation_count']}/{data['total_posts']} "
          f"({data['capitulation_ratio']:.1%})")
    print(f"æ­å°æ–‡: {data['euphoria_count']}/{data['total_posts']} "
          f"({data['euphoria_ratio']:.1%})")

    if data["capitulation_posts"]:
        print("\nç•¢æ¥­æ–‡åˆ—è¡¨:")
        for p in data["capitulation_posts"]:
            print(f"  - {p['title']}")
            print(f"    é—œéµå­—: {', '.join(p['hits'])}")

    if data["euphoria_posts"]:
        print("\næ­å°æ–‡åˆ—è¡¨:")
        for p in data["euphoria_posts"]:
            print(f"  - {p['title']}")
            print(f"    é—œéµå­—: {', '.join(p['hits'])}")


def _print_buzz(data: dict) -> None:
    print(f"\n{'='*90}")
    print("  ç•°å¸¸ç†±åº¦åµæ¸¬ (Buzz Detector)")
    print(f"{'='*90}")

    if data["anomalies"]:
        print("âš ï¸  ç•°å¸¸æ¨™çš„:")
        for a in data["anomalies"]:
            name_str = f" ({a['name']})" if a["name"] else ""
            print(f"  ğŸ”¥ {a['ticker']}{name_str} â€” buzz score: {a['buzz_score']}")
        print()

    print(f"{'æ¨™çš„':<16} {'åç¨±':<12} {'æåŠ':>6} {'Buzz':>8} {'ç•°å¸¸':>6}")
    print("-" * 55)
    for t in data["tickers"][:15]:  # åªé¡¯ç¤ºå‰ 15 å
        name = t["name"][:10] if t["name"] else ""
        flag = "âš ï¸" if t["anomaly"] else ""
        print(f"{t['ticker']:<16} {name:<12} {t['mentions']:>6} {t['buzz_score']:>8.2f} {flag:>6}")


def _print_sectors(data: dict) -> None:
    print(f"\n{'='*90}")
    print("  æ¿å¡Šè¼ªå‹• (Sector Rotation)")
    print(f"{'='*90}")

    if not data["ranking"]:
        print("ï¼ˆæœªåµæ¸¬åˆ°ä»»ä½•æ¿å¡Šé—œéµå­—ï¼‰")
        return

    for i, s in enumerate(data["ranking"], 1):
        bar = "â–ˆ" * min(s["mentions"], 40)
        print(f"  {i:>2}. {s['sector']:<12} {bar} ({s['mentions']})")
        if s["keywords"]:
            print(f"      é—œéµå­—: {', '.join(s['keywords'][:5])}")
        if s["sample_titles"]:
            print(f"      ç¯„ä¾‹: {s['sample_titles'][0][:50]}")


if __name__ == "__main__":
    main()
